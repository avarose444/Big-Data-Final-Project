# ğŸ“° Fake vs Real News Classifier â€” NLP & Deep Learning

## ğŸ“Œ Project Overview

This project explores supervised learning and deep learning techniques to classify news articles as real or fake. Using a labeled Kaggle dataset of 44,000 articles, we applied natural language processing techniques, engineered features based on readability and sentiment, and evaluated a variety of models including LSTM and XGBoost.

The pipeline includes:
- ğŸ§¹ Text preprocessing: cleaning, tokenization, padding
- ğŸ§  Feature engineering (e.g. SMOG Index, subjectivity, sentiment)
- ğŸ§¾ TF-IDF vectorization for sparse feature extraction
- âš™ï¸ Multiple model comparisons: Logistic Regression, XGBoost, LSTM
- ğŸ§ª Evaluation using accuracy, F1 score, and interpretability of features

The goal is to develop a reliable classification system for detecting misinformation using linguistic and semantic patterns in article content.

---

## ğŸ› ï¸ Key Tools & Techniques

- Python (`pandas`, `numpy`)
- NLP preprocessing: `NLTK`, `re`, `string`, `textblob`
- Machine Learning: `scikit-learn`, `XGBoost`
- Deep Learning: `TensorFlow`, `Keras` (LSTM models)
- Evaluation: Confusion Matrix, F1 Score, Accuracy

---

## ğŸ“‚ Dataset

- **Source**: [Kaggle â€“ Fake and Real News Dataset](https://www.kaggle.com/clmentbisaillon/fake-and-real-news-dataset)
- 44,000 articles in two CSV files (`Fake.csv`, `True.csv`)
- Features: `title`, `text`, `subject`, `date`
- Label: 1 (Real), 0 (Fake)

---

## ğŸ“ˆ Summary of Results

| Model                    | Accuracy | F1 Score |
|--------------------------|----------|----------|
| Logistic Regression      | 71%      | 0.69     |
| XGBoost                  | 80%      | 0.79     |
| LSTM                     | 93%      | 0.90     |
| Logistic Regression + LSTM | 97%   | 0.97     |
| XGBoost + LSTM           | **98%**  | **0.98** |

- Fake news articles were more **subjective** and **linguistically complex**
- Real news was **simpler** and had higher **readability**
- LSTM models dramatically improved classification performance

---

## ğŸ§  Feature Engineering

We extracted statistical, semantic, and structural features:
- SMOG Index, Coleman-Liau Index
- Sentence and word length
- Subjectivity and sentiment scores
- TF-IDF vectors for key terms (e.g. â€œReutersâ€ was a strong real-news signal)

---

## ğŸ”¬ Evaluation Techniques

- Train/test split (80/20)
- Confusion matrix analysis
- Precision, recall, and F1 score
- Manual inspection of misclassified articles

---

## ğŸš€ Usage (Coming Soon)

```bash
git clone https://github.com/lsgordon/Big-Data-Final-Project
cd Big-Data-Final-Project
python run_model.py
